{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language translation\n",
    "# Seq2Seq\n",
    "Tutorial on how to converting from english to french using a 'small' database, using seq2seq model APIs in tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.0.0\n"
     ]
    }
   ],
   "source": [
    "# modules to load\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from distutils.version import LooseVersion\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "source_path = 'data/small_vocab_en'\n",
    "target_path = 'data/small_vocab_fr'\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load Dataset from File\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data\n",
    "source_text = load_data(source_path)\n",
    "target_text = load_data(target_path)\n",
    "\n",
    "# Convert all text to lower case\n",
    "source_text = source_text.lower()\n",
    "target_text = target_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets view the source and target examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentences 0 to 5:\n",
      "[0] new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "[1] the united states is usually chilly during july , and it is usually freezing in november .\n",
      "[2] california is usually quiet during march , and it is usually hot in june .\n",
      "[3] the united states is sometimes mild during june , and it is cold in september .\n",
      "[4] your least liked fruit is the grape , but my least liked is the apple .\n",
      "\n",
      "French sentences 0 to 5:\n",
      "[0] new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "[1] the united states is usually chilly during july , and it is usually freezing in november .\n",
      "[2] california is usually quiet during march , and it is usually hot in june .\n",
      "[3] the united states is sometimes mild during june , and it is cold in september .\n",
      "[4] your least liked fruit is the grape , but my least liked is the apple .\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 5)\n",
    "print('English sentences {} to {}:'.format(*view_sentence_range))\n",
    "for i, source in enumerate(source_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]):\n",
    "    print([i],source)\n",
    "\n",
    "print()\n",
    "print('French sentences {} to {}:'.format(*view_sentence_range))\n",
    "for i, source in enumerate(source_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]):\n",
    "    print([i],source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement preprocessing of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has to be converted to IDs which are the list of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary vocab-int pairs (first 10): \n",
      " [('mangoes.', 4), ('lemon', 5), ('animals', 7), ('wonderful', 8), ('horse', 119), ('warm', 9), ('april', 10), ('chinese', 79), ('winter', 206), ('july', 11)]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "# Special characters that we need\n",
    "CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 }\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    \"\"\"\n",
    "    vocab = set(text.split())\n",
    "    vocab_to_int = copy.copy(CODES)\n",
    "\n",
    "    for v_i, v in enumerate(vocab, len(CODES)):\n",
    "        vocab_to_int[v] = v_i\n",
    "\n",
    "    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n",
    "source_vocab_to_int, source_int_to_vocab = create_lookup_tables(source_text)\n",
    "target_vocab_to_int, target_int_to_vocab = create_lookup_tables(target_text)\n",
    "print('Dictionary vocab-int pairs (first 10): \\n', list(source_vocab_to_int.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An French example: \n",
      "\n",
      "[93, 267, 79, 52, 165, 131, 207, 85, 330, 174, 323, 79, 200, 55, 121, 12, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert inputs and targets to int. Add the <EOS> word id at the end of each sentence from target_text. This will help the neural network predict when the sentence should end\n",
    "\n",
    "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Convert source and target text to proper word ids\n",
    "    :param source_text: String that contains all the source text.\n",
    "    :param target_text: String that contains all the target text.\n",
    "    :param source_vocab_to_int: Dictionary to go from the source words to an id\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :return: A tuple of lists (source_id_text, target_id_text)\n",
    "    \"\"\"\n",
    "    \n",
    "    source_text_ids = [[source_vocab_to_int[word] for word in (line).split()] for line in source_text.split('\\n')]\n",
    "    target_text_ids = [[target_vocab_to_int[word] for word in (line + ' <EOS>').split()] for line in target_text.split('\\n')]\n",
    "\n",
    "\n",
    "  \n",
    "    return source_text_ids, target_text_ids\n",
    "source_text_ids, target_text_ids = text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int)\n",
    "\n",
    "print('An French example: \\n')\n",
    "print(target_text_ids[0])\n",
    "target_text.split('\\n')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Lets save the data\n",
    "pickle.dump(((source_text_ids, target_text_ids),\n",
    "        (source_vocab_to_int, target_vocab_to_int),\n",
    "        (source_int_to_vocab, target_int_to_vocab)), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load Data\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = pickle.load(open('preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "The following components are necessary to build a Sequence-to-Sequence model:   \n",
    "- **model_inputs** : A function to recreate placeholders for the neural network\n",
    "- **process_decoding_input**: Remove the last word id in target data and concat the 'GO' ID to the begining of each batch\n",
    "- **encoding_layer** : Create Encoder RNN layer. The encoder hidden state is given to the decoder and have it process its output.\n",
    "- **decoding_layer_train** : We need to declare a decoder for the training phase, and a decoder for the inference/prediction phase. These two decoders will share their parameters (so that all the weights and biases that are set during the training phase can be used when we deploy the model). The training decoder **does not** feed the output of each time step to the next. Rather, the inputs to the decoder time steps are the target sequence from the training dataset (the orange letters).\n",
    "    <img src=\"assets/sequence-to-sequence-training-decoder.png\"/>\n",
    "    \n",
    "    \n",
    "- **decoding_layer_infer** : The inference decoder feeds the output of each time step as an input to the next.\n",
    "    <img src=\"assets/sequence-to-sequence-inference-decoder.png\"/>\n",
    "\n",
    "\n",
    "- **decoding_layer** : Creates a decoder RNN. \n",
    "- **seq2seq_model** : Puts together all the above.\n",
    "\n",
    "### Input\n",
    "The `model_inputs()` function creates TF Placeholders for the Neural Network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    \"\"\"\n",
    "     Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate, keep probability)\n",
    "    \"\"\"\n",
    "\n",
    "    Input = tf.placeholder(tf.int32,[None, None], name='input')\n",
    "    Target = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='lr')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    return Input, Target, lr, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Decoding Input\n",
    "Implement `process_decoding_input` using TensorFlow to remove the last word id from each batch in `target_data` and concat the GO ID to the begining of each batch. (Check the diagram above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GO ID : dict_items([('<GO>', 3)])\n",
      "Input: \n",
      " [[ 0  1  2  3  4  5  6  7  8  9]\n",
      " [10 11 12 13 14 15 16 17 18 19]]\n",
      "\n",
      "\n",
      "Output: \n",
      " [[ 3  0  1  2  3  4  5  6  7  8]\n",
      " [ 3 10 11 12 13 14 15 16 17 18]]\n"
     ]
    }
   ],
   "source": [
    "def process_decoding_input(target_data, target_vocab_to_int, batch_size):\n",
    "    \"\"\"\n",
    "    Preprocess target data for dencoding\n",
    "    :param target_data: Target Placehoder\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :param batch_size: Batch Size\n",
    "    :return: Preprocessed target data\n",
    "    \"\"\"\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    out = tf.concat([tf.fill([batch_size, 1], target_vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return out\n",
    "\n",
    "#Checking the implementation\n",
    "target_vocab_to_int = {'<GO>': 3}\n",
    "batch_size = 2\n",
    "seq_length = 10\n",
    "target_data = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "dec_input = process_decoding_input(target_data, target_vocab_to_int, batch_size)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    demonstration_outputs = np.reshape(range(batch_size * seq_length), (batch_size, seq_length))\n",
    "    print('GO ID :', target_vocab_to_int.items())\n",
    "    print('Input: \\n', demonstration_outputs[:2])\n",
    "    print(\"\\n\")\n",
    "    print('Output: \\n', sess.run(dec_input, {target_data: demonstration_outputs})[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "`encoding_layer()` creates a Encoder RNN layer using [`tf.nn.dynamic_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn). Added the dropout layer on top each cell as it solves overfitting issues. The basic cell used is the LSTM cell. We can construct a stack layer of MultiRNNCell using the 'num_layer' parameter. The encoder_state is return to be passed to the decoder layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob):\n",
    "    \"\"\"\n",
    "    Create encoding layer\n",
    "    :param rnn_inputs: Inputs for the RNN\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: RNN state\n",
    "    \"\"\"\n",
    "\n",
    "    cell = basic_cell = tf.contrib.rnn.DropoutWrapper(\n",
    "        tf.contrib.rnn.BasicLSTMCell(rnn_size),\n",
    "        output_keep_prob=keep_prob)\n",
    "    if num_layers > 1:\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([basic_cell]*num_layers)\n",
    "    \n",
    "    _, enc_state = tf.nn.dynamic_rnn(cell, rnn_inputs, dtype=tf.float32)\n",
    "    return enc_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding - Training\n",
    "Decoder training logits is created using [`tf.contrib.seq2seq.simple_decoder_fn_train()`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/simple_decoder_fn_train) and [`tf.contrib.seq2seq.dynamic_rnn_decoder()`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder).  Apply the `output_fn` to the [`tf.contrib.seq2seq.dynamic_rnn_decoder()`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder) outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope,\n",
    "                         output_fn, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a decoding layer for training\n",
    "    :param encoder_state: Encoder State\n",
    "    :param dec_cell: Decoder RNN Cell\n",
    "    :param dec_embed_input: Decoder embedded input\n",
    "    :param sequence_length: Sequence Length\n",
    "    :param decoding_scope: TenorFlow Variable Scope for decoding\n",
    "    :param output_fn: Function to apply the output layer\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: Train Logits\n",
    "    \"\"\"\n",
    "\n",
    "    train_decoder_fn = tf.contrib.seq2seq.simple_decoder_fn_train(encoder_state)\n",
    "    train_pred, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(\n",
    "        dec_cell, train_decoder_fn, dec_embed_input, sequence_length, scope=decoding_scope)\n",
    "    \n",
    "    # Apply output function\n",
    "    train_logits =  output_fn(train_pred)\n",
    "    \n",
    "    return train_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding - Inference\n",
    "Create inference logits using [`tf.contrib.seq2seq.simple_decoder_fn_inference()`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/simple_decoder_fn_inference) and [`tf.contrib.seq2seq.dynamic_rnn_decoder()`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id,\n",
    "                         maximum_length, vocab_size, decoding_scope, output_fn, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a decoding layer for inference\n",
    "    :param encoder_state: Encoder state\n",
    "    :param dec_cell: Decoder RNN Cell\n",
    "    :param dec_embeddings: Decoder embeddings\n",
    "    :param start_of_sequence_id: GO ID\n",
    "    :param end_of_sequence_id: EOS Id\n",
    "    :param maximum_length: The maximum allowed time steps to decode\n",
    "    :param vocab_size: Size of vocabulary\n",
    "    :param decoding_scope: TensorFlow Variable Scope for decoding\n",
    "    :param output_fn: Function to apply the output layer\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: Inference Logits\n",
    "    \"\"\"\n",
    "\n",
    "    # Inference Decoder\n",
    "    infer_decoder_fn = tf.contrib.seq2seq.simple_decoder_fn_inference(output_fn, encoder_state, dec_embeddings, \n",
    "                                                                      start_of_sequence_id, \n",
    "                                                                      end_of_sequence_id, \n",
    "                                                                      maximum_length, vocab_size)\n",
    "    inference_logits, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, infer_decoder_fn, scope=decoding_scope)\n",
    "    \n",
    "    return inference_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Decoding Layer\n",
    "`decoding_layer()` combines the above to create a Decoder RNN layer.\n",
    "Consits of the following steps:\n",
    "- Create RNN cell for decoding using `rnn_size` and `num_layers`.\n",
    "- Create the output fuction using [`lambda`](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions) to transform it's input, logits, to class logits.\n",
    "- Use the function `decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope, output_fn, keep_prob)` function to get the training logits.\n",
    "- Use the function `decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id, maximum_length, vocab_size, decoding_scope, output_fn, keep_prob)` function to get the inference logits.\n",
    "\n",
    "We need to use [tf.variable_scope](https://www.tensorflow.org/api_docs/python/tf/variable_scope) to share variables between training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size,\n",
    "                   num_layers, target_vocab_to_int, keep_prob):\n",
    "    \"\"\"\n",
    "    Create decoding layer\n",
    "    :param dec_embed_input: Decoder embedded input\n",
    "    :param dec_embeddings: Decoder embeddings\n",
    "    :param encoder_state: The encoded state\n",
    "    :param vocab_size: Size of vocabulary\n",
    "    :param sequence_length: Sequence Length\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: Tuple of (Training Logits, Inference Logits)\n",
    "    \"\"\"\n",
    "\n",
    "    start_of_sequence_id = target_vocab_to_int['<GO>']\n",
    "    end_of_sequence_id = target_vocab_to_int['<EOS>']\n",
    "    \n",
    "    # Decoder RNNs\n",
    "    dec_cell = basic_cell = tf.contrib.rnn.DropoutWrapper(\n",
    "        tf.contrib.rnn.BasicLSTMCell(rnn_size),\n",
    "        output_keep_prob=keep_prob)\n",
    "    if num_layers > 1:\n",
    "        dec_cell = tf.contrib.rnn.MultiRNNCell([basic_cell]*num_layers)\n",
    "    \n",
    "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "        output_fn = lambda x: tf.contrib.layers.fully_connected(x, vocab_size, None, scope=decoding_scope)\n",
    "        train_logits = decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope, output_fn, keep_prob)\n",
    "    \n",
    "    with tf.variable_scope(\"decoding\", reuse=True) as decoding_scope:\n",
    "        inference_logits = decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id, sequence_length, vocab_size, decoding_scope, output_fn, keep_prob)\n",
    "\n",
    "    return train_logits, inference_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Seq2Seq Network\n",
    "Apply the functions implemented above to:\n",
    "\n",
    "- Apply embedding to the input data for the encoder.\n",
    "- Encode the input using your `encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob)`.\n",
    "- Process target data using your `process_decoding_input(target_data, target_vocab_to_int, batch_size)` function.\n",
    "- Apply embedding to the target data for the decoder.\n",
    "- Decode the encoded input using your `decoding_layer(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size, num_layers, target_vocab_to_int, keep_prob)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size, sequence_length, source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size, rnn_size, num_layers, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Build the Sequence-to-Sequence part of the neural network\n",
    "    :param input_data: Input placeholder\n",
    "    :param target_data: Target placeholder\n",
    "    :param keep_prob: Dropout keep probability placeholder\n",
    "    :param batch_size: Batch Size\n",
    "    :param sequence_length: Sequence Length\n",
    "    :param source_vocab_size: Source vocabulary size\n",
    "    :param target_vocab_size: Target vocabulary size\n",
    "    :param enc_embedding_size: Decoder embedding size\n",
    "    :param dec_embedding_size: Encoder embedding size\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :return: Tuple of (Training Logits, Inference Logits)\n",
    "    \"\"\"\n",
    "        \n",
    "    #embedding input data\n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(input_data, source_vocab_size, enc_embedding_size)\n",
    "    encoder_state = encoding_layer(enc_embed_input, rnn_size, num_layers, keep_prob)\n",
    "    \n",
    "    target_data = process_decoding_input(target_data, target_vocab_to_int, batch_size)\n",
    "    \n",
    "    #embedding target data\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, dec_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, target_data)\n",
    "\n",
    "    train_logits, inference_logits = decoding_layer(dec_embed_input, dec_embeddings, encoder_state, target_vocab_size, sequence_length, rnn_size, num_layers, target_vocab_to_int, keep_prob)\n",
    "\n",
    "\n",
    "    return train_logits, inference_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "# Check for a GPU\n",
    "import warnings\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setting up hyperparameters\n",
    "\n",
    "# Number of Epochs\n",
    "epochs = 10\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "# RNN Size\n",
    "rnn_size = 256\n",
    "# Number of Layers\n",
    "num_layers = 2\n",
    "# Embedding Size\n",
    "encoding_embedding_size = 256\n",
    "decoding_embedding_size = 256\n",
    "# Learning Rate\n",
    "learning_rate = 3e-4\n",
    "# Dropout Keep Probability\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph\n",
    "Build the graph using the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_source_sentence_length = max([len(sentence) for sentence in source_int_text])\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = pickle.load(open('preprocess.p', mode='rb'))\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, lr, keep_prob = model_inputs()\n",
    "    sequence_length = tf.placeholder_with_default(max_source_sentence_length, None, name='sequence_length')\n",
    "    input_shape = tf.shape(input_data)\n",
    "    \n",
    "    train_logits, inference_logits = seq2seq_model(\n",
    "        tf.reverse(input_data, [-1]), targets, keep_prob, batch_size, sequence_length, len(source_vocab_to_int), len(target_vocab_to_int),\n",
    "        encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers, target_vocab_to_int)\n",
    "\n",
    "    tf.identity(inference_logits, 'logits')\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            train_logits,\n",
    "            targets,\n",
    "            tf.ones([input_shape[0], sequence_length]))\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "        tf.summary.scalar('loss', cost, collections=['train'])\n",
    "      \n",
    "    s_train = tf.summary.merge_all('train')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train helper functions\n",
    "Helper functions used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PAD input batch\n",
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"\n",
    "    Pad sentence with <PAD> id\n",
    "    \"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [CODES['<PAD>']] * (max_sentence - len(sentence))\n",
    "            for sentence in sentence_batch]\n",
    "\n",
    "def batch_data(source, target, batch_size):\n",
    "    \"\"\"\n",
    "    Batch source and target together\n",
    "    \"\"\"\n",
    "    for batch_i in range(0, len(source)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        source_batch = source[start_i:start_i + batch_size]\n",
    "        target_batch = target[start_i:start_i + batch_size]\n",
    "        yield np.array(pad_sentence_batch(source_batch)), np.array(pad_sentence_batch(target_batch))\n",
    "        \n",
    "# Accuracy calculation\n",
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1]), (0,0)],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, np.argmax(logits, 2)))\n",
    "\n",
    "#Split train validation data and PAD the train data\n",
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "\n",
    "valid_source = pad_sentence_batch(source_int_text[:batch_size])\n",
    "valid_target = pad_sentence_batch(target_int_text[:batch_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "Train the neural network on the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/1077 - Train Accuracy:  0.294, Validation Accuracy:  0.305, Loss:  5.935\n",
      "Epoch   0 Batch  100/1077 - Train Accuracy:  0.415, Validation Accuracy:  0.473, Loss:  2.710\n",
      "Epoch   0 Batch  200/1077 - Train Accuracy:  0.441, Validation Accuracy:  0.502, Loss:  2.159\n",
      "Epoch   0 Batch  300/1077 - Train Accuracy:  0.410, Validation Accuracy:  0.482, Loss:  1.870\n",
      "Epoch   0 Batch  400/1077 - Train Accuracy:  0.423, Validation Accuracy:  0.453, Loss:  1.555\n",
      "Epoch   0 Batch  500/1077 - Train Accuracy:  0.457, Validation Accuracy:  0.483, Loss:  1.383\n",
      "Epoch   0 Batch  600/1077 - Train Accuracy:  0.523, Validation Accuracy:  0.537, Loss:  1.192\n",
      "Epoch   0 Batch  700/1077 - Train Accuracy:  0.511, Validation Accuracy:  0.558, Loss:  1.089\n",
      "Epoch   0 Batch  800/1077 - Train Accuracy:  0.477, Validation Accuracy:  0.565, Loss:  1.035\n",
      "Epoch   0 Batch  900/1077 - Train Accuracy:  0.551, Validation Accuracy:  0.562, Loss:  0.973\n",
      "Epoch   0 Batch 1000/1077 - Train Accuracy:  0.600, Validation Accuracy:  0.581, Loss:  0.853\n",
      "Epoch   1 Batch    0/1077 - Train Accuracy:  0.562, Validation Accuracy:  0.581, Loss:  0.829\n",
      "Epoch   1 Batch  100/1077 - Train Accuracy:  0.594, Validation Accuracy:  0.599, Loss:  0.823\n",
      "Epoch   1 Batch  200/1077 - Train Accuracy:  0.554, Validation Accuracy:  0.597, Loss:  0.792\n",
      "Epoch   1 Batch  300/1077 - Train Accuracy:  0.581, Validation Accuracy:  0.602, Loss:  0.771\n",
      "Epoch   1 Batch  400/1077 - Train Accuracy:  0.592, Validation Accuracy:  0.619, Loss:  0.719\n",
      "Epoch   1 Batch  500/1077 - Train Accuracy:  0.583, Validation Accuracy:  0.620, Loss:  0.684\n",
      "Epoch   1 Batch  600/1077 - Train Accuracy:  0.627, Validation Accuracy:  0.612, Loss:  0.611\n",
      "Epoch   1 Batch  700/1077 - Train Accuracy:  0.609, Validation Accuracy:  0.634, Loss:  0.614\n",
      "Epoch   1 Batch  800/1077 - Train Accuracy:  0.615, Validation Accuracy:  0.620, Loss:  0.610\n",
      "Epoch   1 Batch  900/1077 - Train Accuracy:  0.634, Validation Accuracy:  0.636, Loss:  0.596\n",
      "Epoch   1 Batch 1000/1077 - Train Accuracy:  0.654, Validation Accuracy:  0.634, Loss:  0.518\n",
      "Epoch   2 Batch    0/1077 - Train Accuracy:  0.634, Validation Accuracy:  0.627, Loss:  0.501\n",
      "Epoch   2 Batch  100/1077 - Train Accuracy:  0.662, Validation Accuracy:  0.631, Loss:  0.520\n",
      "Epoch   2 Batch  200/1077 - Train Accuracy:  0.665, Validation Accuracy:  0.639, Loss:  0.506\n",
      "Epoch   2 Batch  300/1077 - Train Accuracy:  0.678, Validation Accuracy:  0.662, Loss:  0.478\n",
      "Epoch   2 Batch  400/1077 - Train Accuracy:  0.659, Validation Accuracy:  0.672, Loss:  0.452\n",
      "Epoch   2 Batch  500/1077 - Train Accuracy:  0.703, Validation Accuracy:  0.683, Loss:  0.412\n",
      "Epoch   2 Batch  600/1077 - Train Accuracy:  0.724, Validation Accuracy:  0.681, Loss:  0.390\n",
      "Epoch   2 Batch  700/1077 - Train Accuracy:  0.684, Validation Accuracy:  0.692, Loss:  0.377\n",
      "Epoch   2 Batch  800/1077 - Train Accuracy:  0.730, Validation Accuracy:  0.700, Loss:  0.370\n",
      "Epoch   2 Batch  900/1077 - Train Accuracy:  0.753, Validation Accuracy:  0.697, Loss:  0.372\n",
      "Epoch   2 Batch 1000/1077 - Train Accuracy:  0.799, Validation Accuracy:  0.732, Loss:  0.322\n",
      "Epoch   3 Batch    0/1077 - Train Accuracy:  0.753, Validation Accuracy:  0.731, Loss:  0.308\n",
      "Epoch   3 Batch  100/1077 - Train Accuracy:  0.770, Validation Accuracy:  0.750, Loss:  0.318\n",
      "Epoch   3 Batch  200/1077 - Train Accuracy:  0.764, Validation Accuracy:  0.752, Loss:  0.305\n",
      "Epoch   3 Batch  300/1077 - Train Accuracy:  0.812, Validation Accuracy:  0.754, Loss:  0.290\n",
      "Epoch   3 Batch  400/1077 - Train Accuracy:  0.788, Validation Accuracy:  0.766, Loss:  0.296\n",
      "Epoch   3 Batch  500/1077 - Train Accuracy:  0.804, Validation Accuracy:  0.766, Loss:  0.253\n",
      "Epoch   3 Batch  600/1077 - Train Accuracy:  0.803, Validation Accuracy:  0.767, Loss:  0.255\n",
      "Epoch   3 Batch  700/1077 - Train Accuracy:  0.804, Validation Accuracy:  0.763, Loss:  0.246\n",
      "Epoch   3 Batch  800/1077 - Train Accuracy:  0.823, Validation Accuracy:  0.800, Loss:  0.234\n",
      "Epoch   3 Batch  900/1077 - Train Accuracy:  0.844, Validation Accuracy:  0.804, Loss:  0.248\n",
      "Epoch   3 Batch 1000/1077 - Train Accuracy:  0.859, Validation Accuracy:  0.788, Loss:  0.207\n",
      "Epoch   4 Batch    0/1077 - Train Accuracy:  0.825, Validation Accuracy:  0.818, Loss:  0.196\n",
      "Epoch   4 Batch  100/1077 - Train Accuracy:  0.843, Validation Accuracy:  0.828, Loss:  0.205\n",
      "Epoch   4 Batch  200/1077 - Train Accuracy:  0.832, Validation Accuracy:  0.812, Loss:  0.211\n",
      "Epoch   4 Batch  300/1077 - Train Accuracy:  0.905, Validation Accuracy:  0.816, Loss:  0.175\n",
      "Epoch   4 Batch  400/1077 - Train Accuracy:  0.889, Validation Accuracy:  0.850, Loss:  0.181\n",
      "Epoch   4 Batch  500/1077 - Train Accuracy:  0.879, Validation Accuracy:  0.840, Loss:  0.159\n",
      "Epoch   4 Batch  600/1077 - Train Accuracy:  0.868, Validation Accuracy:  0.833, Loss:  0.169\n",
      "Epoch   4 Batch  700/1077 - Train Accuracy:  0.904, Validation Accuracy:  0.857, Loss:  0.144\n",
      "Epoch   4 Batch  800/1077 - Train Accuracy:  0.906, Validation Accuracy:  0.865, Loss:  0.145\n",
      "Epoch   4 Batch  900/1077 - Train Accuracy:  0.914, Validation Accuracy:  0.876, Loss:  0.163\n",
      "Epoch   4 Batch 1000/1077 - Train Accuracy:  0.903, Validation Accuracy:  0.881, Loss:  0.132\n",
      "Epoch   5 Batch    0/1077 - Train Accuracy:  0.906, Validation Accuracy:  0.859, Loss:  0.116\n",
      "Epoch   5 Batch  100/1077 - Train Accuracy:  0.900, Validation Accuracy:  0.874, Loss:  0.118\n",
      "Epoch   5 Batch  200/1077 - Train Accuracy:  0.871, Validation Accuracy:  0.887, Loss:  0.126\n",
      "Epoch   5 Batch  300/1077 - Train Accuracy:  0.929, Validation Accuracy:  0.871, Loss:  0.103\n",
      "Epoch   5 Batch  400/1077 - Train Accuracy:  0.920, Validation Accuracy:  0.898, Loss:  0.117\n",
      "Epoch   5 Batch  500/1077 - Train Accuracy:  0.927, Validation Accuracy:  0.887, Loss:  0.095\n",
      "Epoch   5 Batch  600/1077 - Train Accuracy:  0.908, Validation Accuracy:  0.887, Loss:  0.108\n",
      "Epoch   5 Batch  700/1077 - Train Accuracy:  0.932, Validation Accuracy:  0.897, Loss:  0.086\n",
      "Epoch   5 Batch  800/1077 - Train Accuracy:  0.929, Validation Accuracy:  0.900, Loss:  0.096\n",
      "Epoch   5 Batch  900/1077 - Train Accuracy:  0.930, Validation Accuracy:  0.892, Loss:  0.104\n",
      "Epoch   5 Batch 1000/1077 - Train Accuracy:  0.899, Validation Accuracy:  0.874, Loss:  0.085\n",
      "Epoch   6 Batch    0/1077 - Train Accuracy:  0.933, Validation Accuracy:  0.888, Loss:  0.075\n",
      "Epoch   6 Batch  100/1077 - Train Accuracy:  0.916, Validation Accuracy:  0.904, Loss:  0.086\n",
      "Epoch   6 Batch  200/1077 - Train Accuracy:  0.910, Validation Accuracy:  0.922, Loss:  0.088\n",
      "Epoch   6 Batch  300/1077 - Train Accuracy:  0.963, Validation Accuracy:  0.882, Loss:  0.072\n",
      "Epoch   6 Batch  400/1077 - Train Accuracy:  0.930, Validation Accuracy:  0.912, Loss:  0.083\n",
      "Epoch   6 Batch  500/1077 - Train Accuracy:  0.930, Validation Accuracy:  0.916, Loss:  0.064\n",
      "Epoch   6 Batch  600/1077 - Train Accuracy:  0.925, Validation Accuracy:  0.910, Loss:  0.076\n",
      "Epoch   6 Batch  700/1077 - Train Accuracy:  0.941, Validation Accuracy:  0.915, Loss:  0.067\n",
      "Epoch   6 Batch  800/1077 - Train Accuracy:  0.933, Validation Accuracy:  0.893, Loss:  0.078\n",
      "Epoch   6 Batch  900/1077 - Train Accuracy:  0.932, Validation Accuracy:  0.919, Loss:  0.075\n",
      "Epoch   6 Batch 1000/1077 - Train Accuracy:  0.919, Validation Accuracy:  0.907, Loss:  0.067\n",
      "Epoch   7 Batch    0/1077 - Train Accuracy:  0.945, Validation Accuracy:  0.923, Loss:  0.058\n",
      "Epoch   7 Batch  100/1077 - Train Accuracy:  0.938, Validation Accuracy:  0.935, Loss:  0.065\n",
      "Epoch   7 Batch  200/1077 - Train Accuracy:  0.930, Validation Accuracy:  0.925, Loss:  0.067\n",
      "Epoch   7 Batch  300/1077 - Train Accuracy:  0.961, Validation Accuracy:  0.907, Loss:  0.057\n",
      "Epoch   7 Batch  400/1077 - Train Accuracy:  0.936, Validation Accuracy:  0.933, Loss:  0.072\n",
      "Epoch   7 Batch  500/1077 - Train Accuracy:  0.939, Validation Accuracy:  0.913, Loss:  0.055\n",
      "Epoch   7 Batch  600/1077 - Train Accuracy:  0.955, Validation Accuracy:  0.917, Loss:  0.066\n",
      "Epoch   7 Batch  700/1077 - Train Accuracy:  0.943, Validation Accuracy:  0.924, Loss:  0.054\n",
      "Epoch   7 Batch  800/1077 - Train Accuracy:  0.932, Validation Accuracy:  0.933, Loss:  0.063\n",
      "Epoch   7 Batch  900/1077 - Train Accuracy:  0.951, Validation Accuracy:  0.918, Loss:  0.070\n",
      "Epoch   7 Batch 1000/1077 - Train Accuracy:  0.929, Validation Accuracy:  0.915, Loss:  0.065\n",
      "Epoch   8 Batch    0/1077 - Train Accuracy:  0.948, Validation Accuracy:  0.932, Loss:  0.053\n",
      "Epoch   8 Batch  100/1077 - Train Accuracy:  0.939, Validation Accuracy:  0.934, Loss:  0.057\n",
      "Epoch   8 Batch  200/1077 - Train Accuracy:  0.930, Validation Accuracy:  0.930, Loss:  0.058\n",
      "Epoch   8 Batch  300/1077 - Train Accuracy:  0.963, Validation Accuracy:  0.927, Loss:  0.052\n",
      "Epoch   8 Batch  400/1077 - Train Accuracy:  0.954, Validation Accuracy:  0.936, Loss:  0.060\n",
      "Epoch   8 Batch  500/1077 - Train Accuracy:  0.945, Validation Accuracy:  0.941, Loss:  0.044\n",
      "Epoch   8 Batch  600/1077 - Train Accuracy:  0.944, Validation Accuracy:  0.932, Loss:  0.057\n",
      "Epoch   8 Batch  700/1077 - Train Accuracy:  0.954, Validation Accuracy:  0.934, Loss:  0.040\n",
      "Epoch   8 Batch  800/1077 - Train Accuracy:  0.930, Validation Accuracy:  0.935, Loss:  0.049\n",
      "Epoch   8 Batch  900/1077 - Train Accuracy:  0.950, Validation Accuracy:  0.939, Loss:  0.057\n",
      "Epoch   8 Batch 1000/1077 - Train Accuracy:  0.923, Validation Accuracy:  0.926, Loss:  0.054\n",
      "Epoch   9 Batch    0/1077 - Train Accuracy:  0.933, Validation Accuracy:  0.919, Loss:  0.049\n",
      "Epoch   9 Batch  100/1077 - Train Accuracy:  0.946, Validation Accuracy:  0.935, Loss:  0.050\n",
      "Epoch   9 Batch  200/1077 - Train Accuracy:  0.943, Validation Accuracy:  0.936, Loss:  0.052\n",
      "Epoch   9 Batch  300/1077 - Train Accuracy:  0.970, Validation Accuracy:  0.924, Loss:  0.039\n",
      "Epoch   9 Batch  400/1077 - Train Accuracy:  0.954, Validation Accuracy:  0.930, Loss:  0.060\n",
      "Epoch   9 Batch  500/1077 - Train Accuracy:  0.942, Validation Accuracy:  0.939, Loss:  0.040\n",
      "Epoch   9 Batch  600/1077 - Train Accuracy:  0.969, Validation Accuracy:  0.916, Loss:  0.051\n",
      "Epoch   9 Batch  700/1077 - Train Accuracy:  0.949, Validation Accuracy:  0.936, Loss:  0.043\n",
      "Epoch   9 Batch  800/1077 - Train Accuracy:  0.955, Validation Accuracy:  0.935, Loss:  0.038\n",
      "Epoch   9 Batch  900/1077 - Train Accuracy:  0.958, Validation Accuracy:  0.939, Loss:  0.045\n",
      "Epoch   9 Batch 1000/1077 - Train Accuracy:  0.951, Validation Accuracy:  0.934, Loss:  0.043\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "#Use tf.Supervisor to run a managed session\n",
    "import time\n",
    "log_dir = 'log/'\n",
    "sv = tf.train.Supervisor(graph=train_graph, logdir=log_dir, save_model_secs=30)\n",
    "count = 0\n",
    "with sv.managed_session() as sess:\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch) in enumerate(\n",
    "                batch_data(train_source, train_target, batch_size)):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            _, loss, s_t = sess.run(\n",
    "                [train_op, cost,s_train],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 sequence_length: target_batch.shape[1],\n",
    "                 keep_prob: keep_probability})\n",
    "            count += 1\n",
    "            sv.summary_computed(sess, s_t, global_step=count)\n",
    "            \n",
    "            if (batch_i % 100) == 0:\n",
    "                batch_train_logits = sess.run(\n",
    "                    inference_logits, \n",
    "                    {input_data: source_batch, keep_prob: 1.0})\n",
    "                batch_valid_logits= sess.run(\n",
    "                    inference_logits, \n",
    "                    {input_data: valid_source, keep_prob: 1.0})\n",
    "                \n",
    "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "                valid_acc = get_accuracy(np.array(valid_target), batch_valid_logits)\n",
    "                end_time = time.time()\n",
    "            \n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.3f}, Validation Accuracy: {:>6.3f}, Loss: {:>6.3f}'\n",
    "                      .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
    "    sv.saver.save(sess, sv.save_path)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:The above model will be saved every '30' seconds as mentioned in the argument 'save_model_secs'. Try to interuppt the above cell and then start again. The training will start from the last point saved and the model will continue training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard graph \n",
    "View the tensorboard graph using the command: tensorboard --logdir=log. Navigating to http://127.0.1.1:6006 one can check the summaries that have been collected (in our case, inference/loss), as well as the graph of the network, embedding vectors that are trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "## Checkpoint\n",
    "Can start from this checkpoint, the models and data are loaded from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = pickle.load(open('preprocess.p', mode='rb'))\n",
    "load_path = 'log/model.ckpt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence to Sequence\n",
    "To feed a sentence into the model for translation, we need to preprocess it.  Te function `sentence_to_seq()` implements:\n",
    "- Convert the sentence to lowercase\n",
    "- Convert words into ids using `vocab_to_int`\n",
    "- Convert words not in the vocabulary, to the `<UNK>` word id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    \"\"\"\n",
    "    Convert a sentence to a sequence of ids\n",
    "    :param sentence: String\n",
    "    :param vocab_to_int: Dictionary to go from the words to an id\n",
    "    :return: List of word ids\n",
    "    \"\"\"\n",
    "    out = [vocab_to_int.get(word.lower(), vocab_to_int['<UNK>']) for word in sentence.split()]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate\n",
    "This will translate `translate_sentence` from English to French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      "  Word Ids:      [132, 42, 163, 142, 97, 94, 28]\n",
      "  English Words: ['he', 'saw', 'a', 'old', 'yellow', 'truck', '.']\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [323, 195, 142, 199, 67, 234, 12, 1]\n",
      "  French Words: ['il', 'pas', 'un', 'vieux', 'camion', 'jaune', '.', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "translate_sentence = 'he saw a old yellow truck .'\n",
    "\n",
    "translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence], keep_prob: 1.0})[0]\n",
    "\n",
    "    print('Input')\n",
    "    print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "    print('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
    "\n",
    "    print('\\nPrediction')\n",
    "    print('  Word Ids:      {}'.format([i for i in np.argmax(translate_logits, 1)]))\n",
    "    print('  French Words: {}'.format([target_int_to_vocab[i] for i in np.argmax(translate_logits, 1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
