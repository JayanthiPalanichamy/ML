{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing/Exploding gradient problem in rnn\n",
    "\n",
    "Lets analyse the gradients of a simple rnn to understand the problem we encounter during training.\n",
    "\n",
    "<img src='assets/vanish_grad1.png'>\n",
    "\n",
    "RNNs are trained using 'Backpropagation through time (BPTT)', where the objective is to learn the parameters $W_{xh}, W_{hh}, W_{hy}$ by obtaining the error gradients with respect to those parameters through gradient desent.\n",
    "\n",
    "Lets compute the gradient $\\large\\frac{\\partial E} {\\partial W}$ for the above RNN which has three stages. The weights are obtained by summing up the gradients obtained at each time step: $\\sum\\limits_{t}\\large\\frac{\\partial E_{t}} {\\partial W_{hh}}$. But lets only calculate $\\large\\frac{\\partial E_{t+1}} {\\partial W}$ for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do the 'computational graph gradient flow approach' to track the gradients as it flows back in various computational nodes. Go through [Gradient flow in neural network lesson]('http://localhost:8888/notebooks/intro/Gradient%20Flow.ipynb') for the details.\n",
    "\n",
    "The equations involved:   \n",
    "$$\\begin{align}\n",
    "z_{t} &= U x_{t} + W s_{t-1} \\\\    \n",
    "s_{t} &= tanh(z_{t}) \\\\    \n",
    "q_{t} &= V s_{t} \\\\    \n",
    "o_{t} &= softmax(q_{t}) \\\\   \n",
    "\\end{align} $$\n",
    "\n",
    "\n",
    "#### Backpropagating from the last output node:\n",
    "* Since we need to calculate $E_{t+1}$ with respect to $W$, we will start with output node: $o_{t+1}$. Since the weights W is common for all time steps, we need to individualy calculate their gradient for each time step and add the results for the final gradient. Note all the notations below are in matrix form.\n",
    "* ##### W gradient at time (t+1):\n",
    "    + The error gradient $\\large\\frac{\\partial E_{t+1}} {\\partial o_{t+1}}$ is propagated back.\n",
    "    + Next is the activation gate, with output $o_{t+1}$ and input $q_{t+1}$ , so the local gradient $\\large\\frac{\\partial o_{t+1}} {\\partial q_{t+1}}$ gets multiplied with the incoming gradient. Thus we get\n",
    "    \n",
    "    $$\\frac{\\partial E_{t+1}} {\\partial o_{t+1}} . \\frac{\\partial o_{t+1}} {\\partial q_{t+1}} $$\n",
    "        \n",
    "    + Now we encounter the multiplication gate with weights V and $s_{t+1}$, so the gradient towards $s_{t+1}$ is the product of the incoming gradient multiplied by V. The gradient at this point is \n",
    "    \n",
    "    $$\\frac{\\partial E_{t+1}} {\\partial o_{t+1}} . \\frac{\\partial o_{t+1}} {\\partial q_{t+1}} . \\small V $$\n",
    "    \n",
    "    + After that comes the activation unit whose output is $s_{t+1}$ and input is $z_{t+1}$. So the local gradient gets multipled with the incoming gradient. And we get,\n",
    "    \n",
    "    $$\\frac{\\partial E_{t+1}} {\\partial o_{t+1}} . \\frac{\\partial o_{t+1}} {\\partial q_{t+1}} . \\small V . \\frac{\\partial s_{t+1}} {\\partial z_{t+1}} $$\n",
    "    \n",
    "    + Next comes the multiplication gate where the gradient at W is obtained by multiplying the incoming gradient with the 'other input' $s_{t}$, while the gradient that flows through $s_{t}$ gets multiplied with W. So the gradient at W is:   \n",
    "    \n",
    "    $$\\frac{\\partial E_{t+1}} {\\partial W} = \n",
    "    \\frac{\\partial E_{t+1}} {\\partial o_{t+1}} . \n",
    "    \\frac{\\partial o_{t+1}} {\\partial q_{t+1}} . \n",
    "    \\small V . \n",
    "    \\frac{\\partial s_{t+1}} {\\partial z_{t+1}} . \n",
    "    S_{t}$$ \n",
    "    \n",
    "    and the other gradient that flows through $s_{t+1}$ is   \n",
    "    $$\\frac{\\partial E_{t+1}} {\\partial o_{t+1}} . \n",
    "    \\frac{\\partial o_{t+1}} {\\partial q_{t+1}} . \n",
    "    \\small V . \n",
    "    \\frac{\\partial s_{t+1}} {\\partial z_{t+1}} . \n",
    "    \\small W$$\n",
    "\n",
    "* ##### W gradient at time (t):\n",
    "    + Here the next node is the activation unit, so the local gradient gets multiplied with the incoming gradient from the previous step. We get, \n",
    "    \n",
    "    $$\\frac{\\partial E_{t+1}} {\\partial o_{t+1}} . \n",
    "    \\frac{\\partial o_{t+1}} {\\partial q_{t+1}} . \n",
    "    \\small V . \n",
    "    \\frac{\\partial s_{t+1}} {\\partial z_{t+1}} . \n",
    "    \\small W . \n",
    "    \\frac{\\partial s_{t}} {\\partial z_{t}}$$\n",
    "    \n",
    "    + Next comes the multiplication gate where the gradient at W is obtained by multiplying the incoming gradient with the 'other input' $s_{t-1}$, while the gradient that flows through $s_{t-1}$ gets multiplied with W. So the gradient at W is:\n",
    "    \n",
    "     $$\\frac{\\partial E_{t+1}} {\\partial W} =\n",
    "     \\frac{\\partial E_{t+1}} {\\partial o_{t+1}} . \n",
    "    \\frac{\\partial o_{t+1}} {\\partial q_{t+1}} . \n",
    "    \\small V . \n",
    "    \\frac{\\partial s_{t+1}} {\\partial z_{t+1}} . \n",
    "    \\small W . \n",
    "    \\frac{\\partial s_{t}} {\\partial z_{t}}. S_{t-1}$$ \n",
    "     and the other gradient that flows through $s_{t-1}$ is \n",
    "     \n",
    "     $$\\frac{\\partial E_{t+1}} {\\partial o_{t+1}} . \n",
    "    \\frac{\\partial o_{t+1}} {\\partial q_{t+1}} . \n",
    "    \\small V . \n",
    "    \\frac{\\partial s_{t+1}} {\\partial z_{t+1}} . \n",
    "    \\small W . \n",
    "    \\frac{\\partial s_{t}} {\\partial z_{t}}. W$$ \n",
    "\n",
    "* ##### W gradient at time (t-1):\n",
    "    + Repeating the same as above, we get the gradient at W as:\n",
    "    \n",
    "     $$\\frac{\\partial E_{t+1}} {\\partial W} =\n",
    "     \\frac{\\partial E_{t+1}} {\\partial o_{t+1}} . \n",
    "    \\frac{\\partial o_{t+1}} {\\partial q_{t+1}} . \n",
    "    \\small V . \n",
    "    \\frac{\\partial s_{t+1}} {\\partial z_{t+1}} . \n",
    "    \\small W . \n",
    "    \\frac{\\partial s_{t}} {\\partial z_{t}}. W. \\frac{\\partial s_{t}} {\\partial z_{t}}. S_{t-2}$$ \n",
    "    \n",
    "So the final gradient is \n",
    "\n",
    "$$\\begin{align}\\frac{\\partial E_{t+1}} {\\partial W} &= \n",
    "    \\frac{\\partial E_{t+1}} {\\partial o_{t+1}} . \n",
    "    \\frac{\\partial o_{t+1}} {\\partial q_{t+1}} . \n",
    "    \\small V . \n",
    "    \\frac{\\partial s_{t+1}} {\\partial z_{t+1}} . \n",
    "    S_{t}  \\\\ &+ \n",
    "    \\frac{\\partial E_{t+1}} {\\partial o_{t+1}} . \n",
    "    \\frac{\\partial o_{t+1}} {\\partial q_{t+1}} . \n",
    "    \\small V . \n",
    "    \\frac{\\partial s_{t+1}} {\\partial z_{t+1}} . \n",
    "    \\small W . \n",
    "    \\frac{\\partial s_{t}} {\\partial z_{t}}. S_{t-1} \n",
    "    \\\\ &+ \n",
    "    \\frac{\\partial E_{t+1}} {\\partial o_{t+1}} . \n",
    "    \\frac{\\partial o_{t+1}} {\\partial q_{t+1}} . \n",
    "    \\small V . \n",
    "    \\frac{\\partial s_{t+1}} {\\partial z_{t+1}} . \n",
    "    \\small W . \n",
    "    \\frac{\\partial s_{t}} {\\partial z_{t}}. W. \\frac{\\partial s_{t}} {\\partial z_{t}}. S_{t-2} \\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing in python and verifying using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "W = np.array([[0.4, 0.5], [0.1, 0.2]], dtype=np.float32)\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize the weights\n",
    "hidden_dim = 2\n",
    "word_dim = 3\n",
    "bptt_truncate = 3\n",
    "U = np.array([[0.1, 0.2, 0.3],[0.4, 0.5, 0.6]], dtype=np.float32)\n",
    "V = np.array([[0.2, 0.3],[0.5, 0.6],[0.7, 0.8]], dtype=np.float32)\n",
    "W = np.array([[0.4, 0.5], [0.1, 0.2]], dtype=np.float32)\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def forward_propagation(x):\n",
    "    T = len(x)\n",
    "    s = np.zeros((T+1, hidden_dim))\n",
    "    s[-1] = np.zeros(hidden_dim)\n",
    "    o = np.zeros((T, word_dim))\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        s[t] = np.tanh(U.dot(x[t]) + W.dot(s[t-1]))\n",
    "        o[t] = softmax(V.dot(s[t]))\n",
    "    return [o, s]\n",
    "\n",
    "def bptt(x, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    o, s = forward_propagation(x)\n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(U.shape)\n",
    "    dLdV = np.zeros(V.shape)\n",
    "    dLdW = np.zeros(W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        # Initial delta calculation: dL/dz\n",
    "        delta_t = V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-bptt_truncate), t+1)[::-1]:\n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            # Add to gradients at each previous step\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "            dLdU[:,x[bptt_step]] += delta_t\n",
    "            # Update delta for next step dL/dz at t-1\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]\n",
    "\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(xrange(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,3) (2,) (2,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-00a65d228cd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbptt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#o, s = forward_propagation([[0, 1, 2], [1, 2, 3], [4, 5, 6]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-f08dcd94f06c>\u001b[0m in \u001b[0;36mbptt\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# Add to gradients at each previous step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mdLdW\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbptt_step\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mdLdU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbptt_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdelta_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;31m# Update delta for next step dL/dz at t-1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mdelta_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbptt_step\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,3) (2,) (2,3) "
     ]
    }
   ],
   "source": [
    "x = [[0, 1, 0], [1, 0, 0], [0, 0, 1]]\n",
    "y = [[1, 0, 0], [0, 0, 1], [0, 1, 0]]\n",
    "\n",
    "dU, dV, dW = bptt(x, y)\n",
    "#o, s = forward_propagation([[0, 1, 2], [1, 2, 3], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for t in reversed(xrange(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 1 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-c5b73cb04ed9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdelta_o\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for axis 1 with size 3"
     ]
    }
   ],
   "source": [
    "delta_o[np.arange(len(y)), y] -= 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.97233045  0.99800043]\n",
      "[ 0.99943743  0.99999977]\n"
     ]
    }
   ],
   "source": [
    "x = [[0, 1, 2], [1, 2, 3], [4, 5, 6]]\n",
    "z = np.tanh(np.dot(U, x[0]) + np.dot(W, s[-1]))\n",
    "z = np.tanh(np.dot(U, x[1]) + np.dot(W, z))\n",
    "print(z)\n",
    "z = np.tanh(np.dot(U, x[2]) + np.dot(W, z))\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.66403679,  0.93540908])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
