{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********\n",
    " # Understanding convolutional networks - part 2:\n",
    "\n",
    "    1/ Use a set of weights(kernels) obtained from predefined gabor filters\n",
    "    2/ train a convolution network with the predefined filters\n",
    "    3/ Compare it with a convolutional network, with derived weights from back propagation\n",
    "*********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gabor(n_values=7, sigma=1.0, mean=0.0):\n",
    "    x = tf.linspace(-3.0, 3.0, n_values)\n",
    "    z = (tf.exp(tf.negative(tf.pow(x - mean, 2.0) /\n",
    "                       (2.0 * tf.pow(sigma, 2.0)))) *\n",
    "         (1.0 / (sigma * tf.sqrt(2.0 * 3.1415))))\n",
    "    gauss_kernel = tf.matmul(\n",
    "        tf.reshape(z, [n_values, 1]), tf.reshape(z, [1, n_values]))\n",
    "    x = tf.reshape(tf.sin(tf.linspace(-3.0, 3.0, n_values)), [n_values, 1])\n",
    "    y = tf.reshape(tf.ones_like(x), [1, n_values])\n",
    "    gabor_kernel = tf.multiply(tf.matmul(x, y), gauss_kernel)\n",
    "    return gabor_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    out = gabor().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6nby1aTyGMEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe5c21ab70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(out, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nU2IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe5c1f54e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "kernels = []\n",
    "for i, angle in enumerate(range(0, 180, 10)):\n",
    "    rotated = Image.Image.rotate(Image.fromarray(out), angle)\n",
    "    kernels.append(np.array(rotated))\n",
    "plt.imshow(kernels[2], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv2d we will convolve the input with the predefined kernel\n",
    "def conv2d_maxpool(x_tensor, kernels, pool_ksize, pool_strides):\n",
    "    conv = tf.nn.conv2d(x_tensor, kernels, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.relu(conv)\n",
    "    conv = tf.nn.max_pool(conv, ksize = [1, pool_ksize[0], pool_ksize[1], 1],\n",
    "                         strides = [1, pool_strides[0], pool_strides[1], 1],\n",
    "                         padding = 'SAME')\n",
    "    return conv\n",
    "\n",
    "def flatten(x_tensor):\n",
    "    size = x_tensor.get_shape().as_list()\n",
    "    x_tensor = tf.reshape(x_tensor, [-1, size[1]*size[2]*size[3]])\n",
    "    return x_tensor\n",
    "\n",
    "def fully_conn(x_tensor, num_outputs):\n",
    "    size = x_tensor.get_shape().as_list()\n",
    "    weights = tf.Variable(tf.truncated_normal((size[1], num_outputs), stddev=5e-2))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    dense = tf.add(tf.matmul(x_tensor, weights), bias)\n",
    "    out = tf.nn.relu(dense)\n",
    "    return out\n",
    "\n",
    "def output(x_tensor, num_outputs):\n",
    "    size = x_tensor.get_shape().as_list()\n",
    "    weights = tf.Variable(tf.truncated_normal((size[1], num_outputs), stddev=5e-2))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    dense = tf.add(tf.matmul(x_tensor, weights), bias)\n",
    "    return dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Download the MNIST dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a simple convolution network\n",
    "def conv_net(x, kernels, keep_prob):\n",
    "    conv = conv2d_maxpool(x, kernels, (3,3), (2,2))\n",
    "    conv = tf.nn.dropout(conv, keep_prob)\n",
    "    flat = flatten(conv)\n",
    "    dense = fully_conn(flat, 128)\n",
    "    dense = tf.nn.dropout(dense, keep_prob)\n",
    "    out = output(dense, 10)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbdf80875c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The images are in a single vector of 784 (28*28). need to reshape to convert it to a image\n",
    "im = np.reshape(mnist.train.images[2],[28,28])\n",
    "plt.imshow(im, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2900, training accuracy 0.98 \n",
      " Test accuracy 0.945\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Input placeholders\n",
    "x = tf.placeholder(tf.float32,(None, 784), name='x')\n",
    "x_image = tf.reshape(x, [-1, 28,28,1])\n",
    "y = tf.placeholder(tf.float32,(None, 10), name='y' )\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "# The kernel weights are fixed\n",
    "kernels = np.reshape(kernels,[7,7,1,18])\n",
    "weights = tf.constant(kernels, dtype=tf.float32)\n",
    "logits = conv_net(x_image, weights, keep_prob)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(3e-4).minimize(cost)\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(3000):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        optimizer.run(feed_dict={x: batch[0], y: batch[1], keep_prob: 0.5})\n",
    "        if i%100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y:batch[1], keep_prob:1.0})\n",
    "            print(\"step %d, training accuracy %g \"%(i, train_accuracy), end='\\r', flush=True)\n",
    "    accuracy = sess.run(accuracy, feed_dict={x:mnist.validation.images, y:mnist.validation.labels, keep_prob:1.0})\n",
    "    print(\"\\n Test accuracy %g\"%accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********\n",
    "We get a accuracy of more than 0.94 with the hand-coded kernel approach.\n",
    "Now instead of predefining our kernel, let the neural network find its own kernel through backpropagration.\n",
    "********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d_maxpool_w(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \n",
    "    in_channels = x_tensor.get_shape().as_list()[3]\n",
    "    \n",
    "    weight = tf.Variable(tf.truncated_normal((conv_ksize[0], conv_ksize[1], in_channels, conv_num_outputs), stddev=5e-2))\n",
    "\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    \n",
    "    \n",
    "    conv = tf.nn.conv2d(x_tensor, \n",
    "                        weight, \n",
    "                        strides=[1, conv_strides[0], conv_strides[1], 1],\n",
    "                        padding = 'SAME')\n",
    "    conv = tf.nn.bias_add(conv, bias)\n",
    "    conv = tf.nn.relu(conv) \n",
    "    conv_maxpool = tf.nn.max_pool(conv,\n",
    "                                  ksize = [1, pool_ksize[0], pool_ksize[1], 1],\n",
    "                                  strides = [1, pool_strides[0], pool_strides[1], 1],\n",
    "                                  padding = 'SAME')\n",
    "    return conv_maxpool\n",
    "\n",
    "# define a simple convolution network\n",
    "def conv_net_w(x, keep_prob):\n",
    "    conv = conv2d_maxpool_w(x, 18, (7,7),(1,1),(3,3),(2,2))\n",
    "    conv = tf.nn.dropout(conv, keep_prob)\n",
    "    flat = flatten(conv)\n",
    "    dense = fully_conn(flat, 128)\n",
    "    dense = tf.nn.dropout(dense, keep_prob)\n",
    "    out = output(dense, 10)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2900, training accuracy 0.98 \n",
      " Test accuracy 0.9786\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Input placeholders\n",
    "x = tf.placeholder(tf.float32,(None, 784), name='x')\n",
    "x_image = tf.reshape(x, [-1, 28,28,1])\n",
    "y = tf.placeholder(tf.float32,(None, 10), name='y' )\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "logits = conv_net_w(x_image, keep_prob)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(3e-4).minimize(cost)\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(3000):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        optimizer.run(feed_dict={x: batch[0], y: batch[1], keep_prob: 0.5})\n",
    "        if i%100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y:batch[1], keep_prob:1.0})\n",
    "            print(\"step %d, training accuracy %g \"%(i, train_accuracy), end='\\r', flush=True)\n",
    "    accuracy = sess.run(accuracy, feed_dict={x:mnist.validation.images, y:mnist.validation.labels, keep_prob:1.0})\n",
    "    print(\"\\n Test accuracy %g\"%accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "In this case we get a Acuracy of 0.98.\n",
    "This database is easy to train, but if we use a tougher db like CIFAR-10, then we can see the obvious\n",
    "improvement over hand coded kernels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
