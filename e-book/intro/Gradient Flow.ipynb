{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient flow in neural networks\n",
    "[Check the cs231 course for more details](http://cs231n.github.io/optimization-2/)   \n",
    "\n",
    "Understanding backpropagation is important in understanding how deep neural nets work.Given the Loss function L, we are interested in finding the gradients of variables with respect to the loss. Instead of deriving gradients with chain rule, which can be tedious for very large network, we can think of the neural network as a computational graph and figure out how gradient flows at each of the units. Once we know how gradients flow across each of the units, then its simple to grasp backpropagation without writing out derivatives using chain rule. Remember, this is still chain rule at work, but developing this new intuation, will make it easier to undertand really large and complex networks.\n",
    "  \n",
    "Here we look at the nodes a typical neural network is made of: the multiplicative gate, additive gate and the activation gate. \n",
    "\n",
    "<img src='assets/grad_gates1.png'>\n",
    "\n",
    "**Multiplicative gate: **\n",
    "* The incoming gradient is multiplied with inputs switched for calculating their respective gradient. Suppose we want to find the gradient of 'L' with respect to loss 'w', for updating the weights during backpropagation, the gradient is obtained as: $\\large \\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial z} * \\frac{\\partial z }{\\partial w}$, using chain rule we can view it as the incoming gradient $\\large \\frac{\\partial L}{\\partial z}$ gets multiplied with the local gradient $\\large \\frac{\\partial z }{\\partial w}=x$.\n",
    " \n",
    "**Additive gate:**\n",
    "* The incoming gradient is distributed to its inputs. The gradient flows without any changes\n",
    "\n",
    "**Activation gate:**\n",
    "* The incoming gradient is multiplied with the derivative of the activation unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets now look at gradient flow at a node\n",
    "<img src='assets/grad_node.png'>\n",
    "> So calculate the input gradients, the incoming gradient gets multiplied by the 'activation gate' gradient and its passed to both branches of the additive unit and then it gets multiplied with the inputs (reversed).   \n",
    "> So to calculate $\\large \\frac{\\partial L}{\\partial w2}$, the gradient is obtained by the product of three factors: Incoming gradient, the 'local gradient' g'(z)=$\\large \\frac{\\partial g}{\\partial z}$ and 'x'.   \n",
    "> Generally during forward pass the 'local gradient' g'(z) is calculated "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets look at a neural network in terms of computational graph\n",
    "The figure below shows how a neural network is generally represented   \n",
    "\n",
    "<img src='assets/comp_graph1.png'>\n",
    "The above figure is represented in the form of computational units we defined earlier. We dont need to form this everytime we need to undertand a neural network, but we will start with this and we will form simple rules as we progress then we will be able to interpret in the original representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example\n",
    "\n",
    "Let $X=[\\begin{array}{ll}{x1\\\\x2} \\end{array}] = [\\begin{array}{ll}{0.1\\\\0.2} \\end{array}]$,  and the output y = 1.   \n",
    "and let the initial weights be,  \n",
    "\n",
    "$W1=[\\begin{array}{cccc} W1_{11} & W2_{12} \\\\ W1_{21} & W2_{22}\\end{array}]  =   [\\begin{array}{cccc} 0.3 & 0.4 \\\\ 0.5 & 0.6\\end{array}]$,  \n",
    "\n",
    "and $W2=[\\begin{array}{ll}{W2_{11}\\\\W2_{21}} \\end{array}] = [\\begin{array}{ll}{0.7\\\\0.8} \\end{array}]$\n",
    "\n",
    "Then the forward pass will be:   \n",
    "\n",
    "<img src='assets/forward_pass1.png'>\n",
    "\n",
    "\n",
    "Now we want to update the weight $W1_{22}$ using backpropagation, so lets compute the gradient of the Loss (L) with respect to the weight $W1_{22}$\n",
    ":$\\large \\frac{\\partial L}{\\partial W_{22}}$ for updating the weight during backpropagation:\n",
    "* We will start from output Loss function gradient $\\large \\frac{\\partial L}{\\partial y} \\small= \\nabla L = -0.31$.\n",
    "* It meets the activation gate next, so we multiply with $\\large \\frac{\\partial g{2}}{\\partial z} \\small=\\nabla g2=0.2414$, and the result is -0.066\n",
    "* Then it meets the additive gate, so the gradient computed above is just branched \n",
    "* Then the multiplication unit, so we need to multiply the other input $W2_{21}=0.8$. Now the gradient at this node is -0.053\n",
    "* Next comes the activation gate, so we multiply with $\\large \\frac{\\partial g{1\\_2}}{\\partial z}$ and the gradient is -0.0131\n",
    "* It comes to add gate, to the information flows through.\n",
    "* Then we go to $W1_{22}$, here we multiply the other input to this gate x2 with the incoming gradient, thereby obtaining $\\large \\frac{\\partial L}{\\partial W_{22}}$ which is -0.00262.\n",
    "<img src='assets/backprop.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking it in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient at W1_22 is: -0.00262269\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "X = tf.constant([0.1, 0.2], dtype=tf.float32)\n",
    "X = tf.reshape(X,[1,2])\n",
    "W1 = tf.constant([[0.3, 0.4], [0.5, 0.6]], dtype=tf.float32)\n",
    "W2 = tf.constant([0.7, 0.8], dtype=tf.float32)\n",
    "W2 = tf.reshape(W2,[2,1])\n",
    "\n",
    "y = tf.constant(1.0, dtype=tf.float32)\n",
    "z = tf.matmul(X, W1)\n",
    "h = tf.sigmoid(z)\n",
    "out = tf.sigmoid(tf.matmul(h,W2))\n",
    "cost = tf.multiply(tf.square(out-y), 0.5)\n",
    "gradients = tf.gradients(cost, W1)[0]\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "out, grad = sess.run([out, gradients])\n",
    "#print(out)\n",
    "print ('The gradient at W1_22 is:', grad[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the same gradient as worked out before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Lets do the above in the matrix form\n",
    "\n",
    "For a very large network, we cant look at each node individually to do back propagation. Since they are all stacked up together, we will convert all the neural network nodes in matrix form for easier gradient computation.\n",
    "\n",
    "* #### Stacked layers of Multiplicative and additive gate:\n",
    "<img src='assets/mul_add_node.png'>\n",
    "Here we know that the incoming gradients are multiplied by the inputs switched, to obtain the gradients of the inputs. In matrix form they can be written as: $W^{T}* \\nabla (incoming)$ for the input x and $X^{T}* \\nabla (incoming)$ for the weights.     \n",
    "\n",
    "* #### Stacked layers of Activation gate:\n",
    "<img src='assets/act_node.png'>\n",
    "Here we know that the incoming gradients are multiplied by the local gradient, to obtain the gradients of the inputs. Since this is a element by element multiplication, if we need to convert this to matrix form then we need to form a diagonal elements of the activation (local) gradients. \n",
    "\n",
    "## An example\n",
    "\n",
    "<img src='assets/graph.png'>\n",
    "\n",
    "#### We will implement the above graph in both numpy and tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the Variables: x=(3,), y=(1,), W1=(3, 3), W2=(3, 2), W3=(2, 2)\n"
     ]
    }
   ],
   "source": [
    "# Define inputs\n",
    "n_hidden_1 = 3 # first hidden unit size\n",
    "n_hidden_2 = 2 # second hidden unit size\n",
    "\n",
    "X = np.array([0.9, 0.2, -0.7])\n",
    "y = np.array([1. -0.5])\n",
    "W1 = np.array([[-0.3, 0.9, 0.5],[0.8, -0.1, 0.5],[0.8, -0.2, 0.7]])\n",
    "W2 = np.array([[0.7, -0.5],[0.1, -0.5],[-0.6, 0.1]])\n",
    "W3 = np.array([[-0.1, 0.3], [0.2, -0.6]])\n",
    "print('Shapes of the Variables: x={}, y={}, W1={}, W2={}, W3={}'.format(X.shape, y.shape, W1.shape, W2.shape, W3.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (3,) and (2,) not aligned: 3 (dim 0) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-fb67da916f8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdh2\u001b[0m \u001b[0;31m# the global gradient is multiplied with the relu gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mdW2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# gradient for the weight W2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# gradient for the input h1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (3,) and (2,) not aligned: 3 (dim 0) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Numpy Version\n",
    "\n",
    "# sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoidGradient(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def relu(z):\n",
    "    return z.clip(0)\n",
    "\n",
    "def reluGradient(z):\n",
    "    z[np.nonzero(z)] = 1\n",
    "    return z\n",
    "\n",
    "# Forward Pass\n",
    "z1 = np.dot(X, W1)\n",
    "h1 = relu(z1)\n",
    "dh1 = reluGradient(z1) # Calculate the local gradient- derivative of activation\n",
    "    \n",
    "z2 = np.dot(h1, W2)\n",
    "h2 = relu(z2)\n",
    "dh2 = reluGradient(z2)\n",
    "    \n",
    "z3 = np.dot(h2, W3)\n",
    "pred = sigmoid(z3)\n",
    "dh3 = sigmoidGradient(z3)\n",
    "\n",
    "# Backpropagation\n",
    "Error = (1/2.)*(pred-y)**2\n",
    "df = (pred-y) # Error gradient\n",
    "\n",
    "grad = df*dh3 # the global gradient is multiplied with the sigmoid gradient\n",
    "dW3 = np.matmul(h2.T, grad) # gradient for the weight W3\n",
    "grad = np.dot(grad, W3.T) # gradient for the input h2\n",
    "\n",
    "grad = grad*dh2 # the global gradient is multiplied with the relu gradient\n",
    "dW2 = np.dot(h1.T, grad) # gradient for the weight W2\n",
    "grad = np.dot(grad, W2.T) # gradient for the input h1\n",
    "\n",
    "grad = grad*dh1 # the global gradient is multiplied with the relu gradient\n",
    "dW1 = np.dot(X, grad) # gradient for the weight W1\n",
    "\n",
    "print('The gradient of the weights, dW1:', dW1)\n",
    "print('The gradient of the weights, dW2:', dW2)\n",
    "print('The gradient of the weights, dW3:', dW3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.0306030084830415e-05"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.035683549500680388"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
